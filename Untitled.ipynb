{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ce48889",
   "metadata": {},
   "source": [
    "TF-IDF (Term Frequency-Inverse Document Frequency) is a numerical statistic used to reflect the importance of a word in a document relative to a collection of documents (or corpus). It is often used in text mining and information retrieval to evaluate how relevant a word is in a particular document. Here's a breakdown of how TF-IDF works:\n",
    "\n",
    "### Term Frequency (TF)\n",
    "\n",
    "**Term Frequency** measures how frequently a term (word) appears in a document. It is calculated as follows:\n",
    "\n",
    "\\[ \\text{TF}(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d} \\]\n",
    "\n",
    "The more frequently a term appears in a document, the higher its TF value.\n",
    "\n",
    "### Inverse Document Frequency (IDF)\n",
    "\n",
    "**Inverse Document Frequency** measures the importance of a term in the corpus. It is calculated as follows:\n",
    "\n",
    "\\[ \\text{IDF}(t, D) = \\log \\left( \\frac{\\text{Total number of documents } N}{\\text{Number of documents containing term } t} \\right) \\]\n",
    "\n",
    "The idea is that if a term appears in many documents, it is less important (common words like \"the\", \"is\", etc.). The IDF value decreases as the term appears in more documents.\n",
    "\n",
    "### TF-IDF Calculation\n",
    "\n",
    "The **TF-IDF** score is the product of the TF and IDF scores for a term. It is calculated as follows:\n",
    "\n",
    "\\[ \\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D) \\]\n",
    "\n",
    "Where:\n",
    "- \\( t \\) is the term.\n",
    "- \\( d \\) is an individual document.\n",
    "- \\( D \\) is the entire document corpus.\n",
    "- \\( N \\) is the total number of documents in the corpus.\n",
    "- The logarithm in IDF can be natural log (ln) or base 10 log (log10).\n",
    "\n",
    "### Example\n",
    "\n",
    "Let's go through a simple example to illustrate the calculation:\n",
    "\n",
    "1. **Document Corpus**:\n",
    "   - Document 1 (d1): \"this is a sample\"\n",
    "   - Document 2 (d2): \"this is another example example\"\n",
    "\n",
    "2. **Calculate Term Frequency (TF)**:\n",
    "   - For term \"this\" in d1: \n",
    "     \\[ \\text{TF}(\"this\", d1) = \\frac{1}{4} = 0.25 \\]\n",
    "   - For term \"example\" in d2:\n",
    "     \\[ \\text{TF}(\"example\", d2) = \\frac{2}{4} = 0.5 \\]\n",
    "\n",
    "3. **Calculate Document Frequency (DF)**:\n",
    "   - \"this\" appears in both documents, so DF(\"this\") = 2.\n",
    "   - \"example\" appears in one document, so DF(\"example\") = 1.\n",
    "\n",
    "4. **Calculate Inverse Document Frequency (IDF)**:\n",
    "   - For term \"this\":\n",
    "     \\[ \\text{IDF}(\"this\", D) = \\log \\left( \\frac{2}{2} \\right) = \\log (1) = 0 \\]\n",
    "   - For term \"example\":\n",
    "     \\[ \\text{IDF}(\"example\", D) = \\log \\left( \\frac{2}{1} \\right) = \\log (2) \\approx 0.693 \\]\n",
    "\n",
    "5. **Calculate TF-IDF**:\n",
    "   - For term \"this\" in d1:\n",
    "     \\[ \\text{TF-IDF}(\"this\", d1, D) = 0.25 \\times 0 = 0 \\]\n",
    "   - For term \"example\" in d2:\n",
    "     \\[ \\text{TF-IDF}(\"example\", d2, D) = 0.5 \\times 0.693 \\approx 0.346 \\]\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **TF-IDF** provides a weight that balances the frequency of a term in a document against how common the term is across the entire corpus.\n",
    "- Common words across many documents have low TF-IDF scores, while words unique to a few documents have higher scores.\n",
    "- This method helps to highlight the most important terms within a document relative to a larger set of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf2bfd3",
   "metadata": {},
   "source": [
    "Limitations of TF-IDF\n",
    "Context Ignorance: TF-IDF does not capture the semantic meaning of words. Words with similar meanings but different forms will be treated differently.\n",
    "Static Weights: The weights are static and do not account for the dynamic nature of language and context.\n",
    "Scalability: For very large corpora, computing TF-IDF can become computationally intensive.\n",
    "Despite these limitations, TF-IDF remains a foundational technique in text processing and continues to be widely used due to its simplicity and effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6de03a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f63d36c",
   "metadata": {},
   "source": [
    "A Count Vectorizer is a technique used in Natural Language Processing (NLP) to convert a collection of text documents into a matrix of token counts. It is a fundamental tool for text preprocessing and is often used as a first step in transforming raw text into a format suitable for machine learning algorithms.\n",
    "\n",
    "### How Count Vectorizer Works\n",
    "\n",
    "1. **Tokenization**: The text documents are tokenized, meaning they are split into individual words or tokens.\n",
    "2. **Vocabulary Building**: A vocabulary (set of unique tokens) is built from all the documents.\n",
    "3. **Count Matrix**: A matrix is created where each row represents a document and each column represents a token from the vocabulary. The values in the matrix are the counts of the tokens in each document.\n",
    "\n",
    "### Example\n",
    "\n",
    "Consider a simple example with three documents:\n",
    "\n",
    "1. Document 1: \"I love programming\"\n",
    "2. Document 2: \"Programming is fun\"\n",
    "3. Document 3: \"I love fun activities\"\n",
    "\n",
    "#### Tokenization and Vocabulary Building\n",
    "\n",
    "From these documents, the vocabulary would be:\n",
    "\n",
    "\\[ \\text{\"I\", \"love\", \"programming\", \"is\", \"fun\", \"activities\"} \\]\n",
    "\n",
    "#### Count Matrix\n",
    "\n",
    "The count matrix for these documents would be:\n",
    "\n",
    "\\[\n",
    "\\begin{array}{l|c|c|c|c|c|c}\n",
    " & \\text{\"I\"} & \\text{\"love\"} & \\text{\"programming\"} & \\text{\"is\"} & \\text{\"fun\"} & \\text{\"activities\"} \\\\\n",
    "\\hline\n",
    "\\text{Document 1} & 1 & 1 & 1 & 0 & 0 & 0 \\\\\n",
    "\\text{Document 2} & 0 & 0 & 1 & 1 & 1 & 0 \\\\\n",
    "\\text{Document 3} & 1 & 1 & 0 & 0 & 1 & 1 \\\\\n",
    "\\end{array}\n",
    "\\]\n",
    "\n",
    "### Applications\n",
    "\n",
    "- **Text Classification**: Used as input features for machine learning models to classify text into categories (e.g., spam detection, sentiment analysis).\n",
    "- **Information Retrieval**: Helps in retrieving documents that are relevant to a user's query by counting term occurrences.\n",
    "- **Text Similarity**: Used to measure the similarity between documents by comparing their count vectors.\n",
    "\n",
    "### Advantages\n",
    "\n",
    "- **Simplicity**: Easy to understand and implement.\n",
    "- **Effectiveness**: Provides a straightforward way to represent text data numerically.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- **Context Ignorance**: Does not capture the meaning or context of the words. Words with similar meanings but different forms will be treated differently.\n",
    "- **High Dimensionality**: The resulting count matrix can be very large, especially with a large vocabulary, leading to sparsity issues.\n",
    "- **Frequency Bias**: Frequent words (e.g., \"the\", \"is\") may dominate the count matrix, potentially overshadowing more informative terms.\n",
    "\n",
    "### Python Implementation using Scikit-Learn\n",
    "\n",
    "Here's a simple implementation using the `CountVectorizer` from the `scikit-learn` library:\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"I love programming\",\n",
    "    \"Programming is fun\",\n",
    "    \"I love fun activities\"\n",
    "]\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert the result to an array\n",
    "count_matrix = X.toarray()\n",
    "\n",
    "# Get the feature names (vocabulary)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"Vocabulary:\", feature_names)\n",
    "print(\"Count Matrix:\\n\", count_matrix)\n",
    "```\n",
    "\n",
    "### Output\n",
    "\n",
    "```\n",
    "Vocabulary: ['activities' 'fun' 'is' 'love' 'programming']\n",
    "Count Matrix:\n",
    " [[0 0 0 1 1 1]\n",
    "  [0 1 1 0 1 0]\n",
    "  [1 1 0 1 0 1]]\n",
    "```\n",
    "\n",
    "This example demonstrates how Count Vectorizer converts text documents into a matrix of token counts, which can then be used for various NLP and machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc670277",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
