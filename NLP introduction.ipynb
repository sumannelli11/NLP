{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c892586",
   "metadata": {},
   "source": [
    "1. Introduction to NLP\n",
    "Definition: Understanding what NLP is and its significance in the field of artificial intelligence.\n",
    "NLP is a subfield of AI that focuses on the interaction between computers and humans through natural language.\n",
    "It involves the application of computational techniques to analyze and synthesize natural language and speech.\n",
    "2. Applications of NLP\n",
    "Real-world Applications: Discussing various practical applications of NLP.\n",
    "Text Classification: Spam detection, sentiment analysis, and news categorization.\n",
    "Machine Translation: Google Translate and other translation services.\n",
    "Information Retrieval: Search engines and question-answering systems.\n",
    "Text Summarization: Automatic summarization of articles and documents.\n",
    "Speech Recognition: Virtual assistants like Siri and Alexa.\n",
    "Chatbots and Conversational Agents: Customer service bots and personal assistants.\n",
    "3. Fundamental Concepts in NLP\n",
    "Tokenization: Splitting text into words, phrases, symbols, or other meaningful elements called tokens.\n",
    "Part-of-Speech Tagging (POS Tagging): Identifying the part of speech for each token in a sentence (e.g., noun, verb, adjective).\n",
    "Named Entity Recognition (NER): Detecting and classifying named entities such as people, organizations, locations, dates, and more.\n",
    "Syntax and Parsing: Analyzing the grammatical structure of sentences.\n",
    "Semantic Analysis: Understanding the meaning and context of words and sentences.\n",
    "Word Embeddings: Representing words as vectors in a continuous vector space to capture their meanings and relationships (e.g., Word2Vec, GloVe).\n",
    "4. Key Techniques in NLP\n",
    "Statistical Methods: Leveraging probabilistic models to understand and generate language (e.g., Hidden Markov Models, N-grams).\n",
    "Machine Learning: Applying supervised and unsupervised learning techniques to NLP tasks.\n",
    "Supervised Learning: Using labeled data to train models (e.g., text classification).\n",
    "Unsupervised Learning: Discovering hidden patterns in unlabeled data (e.g., topic modeling).\n",
    "Deep Learning: Using neural networks for more complex NLP tasks.\n",
    "Recurrent Neural Networks (RNNs): Handling sequential data, particularly useful for language modeling.\n",
    "Transformers: State-of-the-art models for NLP tasks (e.g., BERT, GPT).\n",
    "5. Tools and Libraries for NLP\n",
    "NLTK (Natural Language Toolkit): A comprehensive library for various NLP tasks.\n",
    "spaCy: An open-source software library for advanced NLP.\n",
    "Stanford NLP: A suite of NLP tools provided by Stanford University.\n",
    "Hugging Face's Transformers: A library that provides pre-trained models and tools for building NLP applications.\n",
    "6. Basic NLP Pipeline\n",
    "Text Preprocessing: Cleaning and preparing text data for analysis.\n",
    "Lowercasing: Converting text to lowercase for uniformity.\n",
    "Removing Punctuation and Stop Words: Eliminating unnecessary symbols and common words.\n",
    "Stemming and Lemmatization: Reducing words to their root forms.\n",
    "Feature Extraction: Converting text into numerical features.\n",
    "Bag of Words (BoW): Representing text by the frequency of words.\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency): Weighing terms based on their importance in a document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bec6781",
   "metadata": {},
   "source": [
    "Text processing and tokenization are foundational steps in any NLP project. These processes convert raw text into a format that can be analyzed and used by machine learning models. Here's a detailed overview of basic text processing and tokenization.\n",
    "\n",
    "1. Basic Text Processing\n",
    "Text processing involves cleaning and preparing text data for analysis. This step is crucial for reducing noise and making the data more uniform. Common text processing tasks include:\n",
    "\n",
    "Lowercasing: Converting all characters in the text to lowercase.\n",
    "Example: \"This is an Example.\" → \"this is an example.\"\n",
    "\n",
    "Removing Punctuation: Eliminating punctuation marks from the text.\n",
    "Example: \"Hello, world!\" → \"Hello world\"\n",
    "\n",
    "Removing Stop Words: Removing common words that do not carry significant meaning, such as \"and\", \"is\", \"in\", etc.\n",
    "Example: \"This is an example of text processing.\" → \"example text processing\"\n",
    "\n",
    "Stemming: Reducing words to their base or root form by removing suffixes.\n",
    "Example: \"running\", \"runs\", \"ran\" → \"run\"\n",
    "\n",
    "Lemmatization: Converting words to their base or dictionary form, considering the context.\n",
    "Example: \"better\" → \"good\", \"running\" → \"run\"\n",
    "\n",
    "Removing Numbers: Eliminating numerical values from the text.\n",
    "Example: \"There are 123 apples.\" → \"There are apples\"\n",
    "\n",
    "Whitespace Removal: Removing extra spaces, tabs, and newlines from the text.\n",
    "Example: \"Hello world\" → \"Hello world\"\n",
    "\n",
    "2. Tokenization\n",
    "\n",
    "Tokenization is the process of splitting text into smaller units, called tokens. These tokens can be words, phrases, or symbols, and are the basic building blocks for further NLP tasks.\n",
    "\n",
    "Types of Tokenization:\n",
    "Word Tokenization: Splitting text into individual words.\n",
    "\n",
    "Example: \"Hello world!\" → [\"Hello\", \"world\"]\n",
    "Sentence Tokenization: Splitting text into sentences.\n",
    "\n",
    "Example: \"Hello world! How are you?\" → [\"Hello world!\", \"How are you?\"]\n",
    "Subword Tokenization: Splitting text into subwords or characters, useful for handling unknown words and morphologically rich languages.\n",
    "\n",
    "Example: \"unhappiness\" → [\"un\", \"happiness\"]\n",
    "Tokenization Techniques:\n",
    "Whitespace Tokenization: Splitting text based on spaces.\n",
    "\n",
    "Example: \"Tokenize this text.\" → [\"Tokenize\", \"this\", \"text\"]\n",
    "Regex Tokenization: Using regular expressions to define token boundaries.\n",
    "\n",
    "Example: Using \\W+ to split on non-word characters: \"Hello, world!\" → [\"Hello\", \"world\"]\n",
    "Library-based Tokenization: Utilizing NLP libraries like NLTK, spaCy, or Hugging Face Transformers for advanced tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a29507b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens: ['Hello', 'world', '!', 'How', 'are', 'you', 'how', 'are', 'youdoing']\n",
      "Sentence Tokens: ['Hello world!', 'How are you how are youdoing']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "text = \"Hello world! How are you how are youdoing\"\n",
    "word_tokens = word_tokenize(text)\n",
    "sent_tokens = sent_tokenize(text)\n",
    "\n",
    "print(\"Word Tokens:\", word_tokens)\n",
    "print(\"Sentence Tokens:\", sent_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c60e7eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_40228\\3767629988.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"en_core_web_sm\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Hello world! How are you?\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\practice\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0menable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m     )\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\practice\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[index]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 472\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    473\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Hello world! How are you?\")\n",
    "\n",
    "word_tokens = [token.text for token in doc]\n",
    "sent_tokens = [sent.text for sent in doc.sents]\n",
    "\n",
    "print(\"Word Tokens:\", word_tokens)\n",
    "print(\"Sentence Tokens:\", sent_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38002a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'processing', 'is', 'crucial', 'in', 'nlp', 'this', 'includes', 'tokenization', 'stemming', 'and', 'lemmatization']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Sample text\n",
    "text = \"Text processing is crucial in NLP. This includes tokenization, stemming, and lemmatization.\"\n",
    "\n",
    "# Lowercasing\n",
    "text = text.lower()\n",
    "\n",
    "# Removing punctuation\n",
    "import re\n",
    "text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e2f8e09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text',\n",
       " 'processing',\n",
       " 'crucial',\n",
       " 'nlp',\n",
       " 'includes',\n",
       " 'tokenization',\n",
       " 'stemming',\n",
       " 'lemmatization']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Removing stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "filtered_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13e27b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text', 'process', 'crucial', 'nlp', 'includ', 'token', 'stem', 'lemmat']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming\n",
    "ps = PorterStemmer()\n",
    "stemmed_tokens = [ps.stem(word) for word in filtered_tokens]\n",
    "\n",
    "stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fff5193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Tokens: ['text', 'processing', 'crucial', 'nlp', 'includes', 'tokenization', 'stemming', 'lemmatization']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "\n",
    "print(\"Lemmatized Tokens:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675d0345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caafb66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9553ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44f13f9e",
   "metadata": {},
   "source": [
    "Summary\n",
    "Text Processing: Cleaning and preparing text data through lowercasing, removing punctuation, stop words, numbers, and performing stemming and lemmatization.\n",
    "Tokenization: Splitting text into tokens (words, sentences, or subwords) for further analysis.\n",
    "Practical Tools: Using libraries like NLTK and spaCy for efficient and advanced text processing and tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72294cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8130498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f928a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fce95a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
