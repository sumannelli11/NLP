{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "386497ce",
   "metadata": {},
   "source": [
    "spaCy is a powerful and fast library for advanced Natural Language Processing (NLP) in Python. It provides a wide range of functionalities for processing and analyzing text. Below is an overview of some of the key operations you can perform with spaCy, along with examples for each operation.\n",
    "\n",
    "### 1. Installation\n",
    "\n",
    "First, you need to install spaCy and download a language model:\n",
    "\n",
    "```bash\n",
    "pip install spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "### 2. Loading the Language Model\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "```\n",
    "\n",
    "### 3. Tokenization\n",
    "\n",
    "Tokenization is the process of breaking text into individual words or tokens.\n",
    "\n",
    "```python\n",
    "doc = nlp(\"SpaCy is an excellent NLP library.\")\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "```\n",
    "\n",
    "### 4. Part-of-Speech (POS) Tagging\n",
    "\n",
    "Identifying the part of speech for each token.\n",
    "\n",
    "```python\n",
    "for token in doc:\n",
    "    print(f\"{token.text} - {token.pos_}\")\n",
    "```\n",
    "\n",
    "### 5. Named Entity Recognition (NER)\n",
    "\n",
    "Identifying named entities in the text.\n",
    "\n",
    "```python\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text} - {ent.label_}\")\n",
    "```\n",
    "\n",
    "### 6. Dependency Parsing\n",
    "\n",
    "Analyzing the syntactic structure of the sentence.\n",
    "\n",
    "```python\n",
    "for token in doc:\n",
    "    print(f\"{token.text} - {token.dep_} - {token.head.text}\")\n",
    "```\n",
    "\n",
    "### 7. Lemmatization\n",
    "\n",
    "Finding the base form of each word.\n",
    "\n",
    "```python\n",
    "for token in doc:\n",
    "    print(f\"{token.text} - {token.lemma_}\")\n",
    "```\n",
    "\n",
    "### 8. Stop Words\n",
    "\n",
    "Identifying and removing stop words.\n",
    "\n",
    "```python\n",
    "for token in doc:\n",
    "    if not token.is_stop:\n",
    "        print(token.text)\n",
    "```\n",
    "\n",
    "### 9. Text Similarity\n",
    "\n",
    "Calculating similarity between two texts.\n",
    "\n",
    "```python\n",
    "doc1 = nlp(\"I love cats\")\n",
    "doc2 = nlp(\"I love dogs\")\n",
    "similarity = doc1.similarity(doc2)\n",
    "print(f\"Similarity: {similarity}\")\n",
    "```\n",
    "\n",
    "### 10. Visualizing Dependencies\n",
    "\n",
    "Using displaCy for visualizing syntactic dependencies.\n",
    "\n",
    "```python\n",
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"dep\")\n",
    "```\n",
    "\n",
    "### 11. Custom Named Entity Recognition\n",
    "\n",
    "Adding custom named entities to the NER pipeline.\n",
    "\n",
    "```python\n",
    "from spacy.tokens import Span\n",
    "\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "org = doc[0:1]\n",
    "money = doc[8:11]\n",
    "org_label = Span(doc, 0, 1, label=\"ORG\")\n",
    "money_label = Span(doc, 8, 11, label=\"MONEY\")\n",
    "doc.ents = list(doc.ents) + [org_label, money_label]\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text} - {ent.label_}\")\n",
    "```\n",
    "\n",
    "### 12. Rule-Based Matching\n",
    "\n",
    "Using the Matcher class to find patterns in the text.\n",
    "\n",
    "```python\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"LOWER\": \"nlp\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"library\"}]\n",
    "matcher.add(\"NLP_LIBRARY_PATTERN\", [pattern])\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(span.text)\n",
    "```\n",
    "\n",
    "### 13. Sentence Segmentation\n",
    "\n",
    "Segmenting text into sentences.\n",
    "\n",
    "```python\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)\n",
    "```\n",
    "\n",
    "### 14. Word Vectors and Similarity\n",
    "\n",
    "Accessing word vectors and calculating similarity between words.\n",
    "\n",
    "```python\n",
    "token1 = nlp(\"cat\")[0]\n",
    "token2 = nlp(\"dog\")[0]\n",
    "similarity = token1.similarity(token2)\n",
    "print(f\"Word Similarity: {similarity}\")\n",
    "```\n",
    "\n",
    "### 15. Custom Components in Pipeline\n",
    "\n",
    "Adding custom components to the spaCy processing pipeline.\n",
    "\n",
    "```python\n",
    "def custom_component(doc):\n",
    "    print(\"Custom component executed\")\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(custom_component, last=True)\n",
    "doc = nlp(\"SpaCy is an excellent NLP library.\")\n",
    "```\n",
    "\n",
    "### 16. Text Classification\n",
    "\n",
    "Training and using text classification models (requires more extensive setup, including training data).\n",
    "\n",
    "```python\n",
    "# Placeholder for training a text classifier\n",
    "# More details would be needed for a full example\n",
    "```\n",
    "\n",
    "### 17. Extending Token and Doc Objects\n",
    "\n",
    "Adding custom attributes to tokens or documents.\n",
    "\n",
    "```python\n",
    "from spacy.tokens import Token\n",
    "\n",
    "Token.set_extension(\"is_custom\", default=False)\n",
    "\n",
    "token = nlp(\"Hello\")[0]\n",
    "token._.is_custom = True\n",
    "print(token._.is_custom)\n",
    "```\n",
    "\n",
    "These are some of the core functionalities of spaCy. Each of these operations can be further customized and combined to build complex NLP applications. For more detailed use cases and advanced configurations, refer to the official spaCy documentation at [spacy.io](https://spacy.io/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b6cab27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpaCy is an excellent NLP library. Apple is looking at buying U.K. startup for $1 billion."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span, Token\n",
    "from spacy.matcher import Matcher\n",
    "from spacy import displacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = \"SpaCy is an excellent NLP library. Apple is looking at buying U.K. startup for $1 billion.\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7fb24bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization:\n",
      "SpaCy\n",
      "is\n",
      "an\n",
      "excellent\n",
      "NLP\n",
      "library\n",
      ".\n",
      "Apple\n",
      "is\n",
      "looking\n",
      "at\n",
      "buying\n",
      "U.K.\n",
      "startup\n",
      "for\n",
      "$\n",
      "1\n",
      "billion\n",
      ".\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Tokenization\n",
    "print(\"Tokenization:\")\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "042999e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagging:\n",
      "SpaCy - PROPN\n",
      "is - AUX\n",
      "an - DET\n",
      "excellent - ADJ\n",
      "NLP - PROPN\n",
      "library - NOUN\n",
      ". - PUNCT\n",
      "Apple - PROPN\n",
      "is - AUX\n",
      "looking - VERB\n",
      "at - ADP\n",
      "buying - VERB\n",
      "U.K. - PROPN\n",
      "startup - NOUN\n",
      "for - ADP\n",
      "$ - SYM\n",
      "1 - NUM\n",
      "billion - NUM\n",
      ". - PUNCT\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2. Part-of-Speech (POS) Tagging\n",
    "print(\"POS Tagging:\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text} - {token.pos_}\")\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71eacba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entity Recognition:\n",
      "NLP - ORG\n",
      "Apple - ORG\n",
      "U.K. - GPE\n",
      "$1 billion - MONEY\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3. Named Entity Recognition (NER)\n",
    "print(\"Named Entity Recognition:\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text} - {ent.label_}\")\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a5db37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency Parsing:\n",
      "SpaCy - nsubj - is\n",
      "is - ROOT - is\n",
      "an - det - library\n",
      "excellent - amod - library\n",
      "NLP - compound - library\n",
      "library - attr - is\n",
      ". - punct - is\n",
      "Apple - nsubj - looking\n",
      "is - aux - looking\n",
      "looking - ROOT - looking\n",
      "at - prep - looking\n",
      "buying - pcomp - at\n",
      "U.K. - dobj - buying\n",
      "startup - dep - looking\n",
      "for - prep - startup\n",
      "$ - quantmod - billion\n",
      "1 - compound - billion\n",
      "billion - pobj - for\n",
      ". - punct - looking\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4. Dependency Parsing\n",
    "print(\"Dependency Parsing:\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text} - {token.dep_} - {token.head.text}\")\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24f0e0c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization:\n",
      "SpaCy - SpaCy\n",
      "is - be\n",
      "an - an\n",
      "excellent - excellent\n",
      "NLP - NLP\n",
      "library - library\n",
      ". - .\n",
      "Apple - Apple\n",
      "is - be\n",
      "looking - look\n",
      "at - at\n",
      "buying - buy\n",
      "U.K. - U.K.\n",
      "startup - startup\n",
      "for - for\n",
      "$ - $\n",
      "1 - 1\n",
      "billion - billion\n",
      ". - .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5. Lemmatization\n",
    "print(\"Lemmatization:\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text} - {token.lemma_}\")\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bde30127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Words:\n",
      "SpaCy\n",
      "excellent\n",
      "NLP\n",
      "library\n",
      ".\n",
      "Apple\n",
      "looking\n",
      "buying\n",
      "U.K.\n",
      "startup\n",
      "$\n",
      "1\n",
      "billion\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 6. Stop Words\n",
    "print(\"Stop Words:\")\n",
    "for token in doc:\n",
    "    if not token.is_stop:\n",
    "        print(token.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09ef4273",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Text Similarity: 0.16777146679017096\n",
      "\n",
      "Visualizing Dependencies:\n",
      "Custom Named Entity Recognition:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suman\\.conda\\envs\\practice\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E1010] Unable to set entity information for token 7 which is included in more than one span in entities, blocked, missing or outside.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_38604\\254997139.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0morg_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSpan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ORG\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mmoney_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSpan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"MONEY\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ments\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0morg_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmoney_label\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ment\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ments\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{ent.text} - {ent.label_}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\practice\\lib\\site-packages\\spacy\\tokens\\doc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.ents.__set__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\practice\\lib\\site-packages\\spacy\\tokens\\doc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.set_ents\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E1010] Unable to set entity information for token 7 which is included in more than one span in entities, blocked, missing or outside."
     ]
    }
   ],
   "source": [
    "print(\"\\n\")\n",
    "\n",
    "# 7. Text Similarity\n",
    "doc1 = nlp(\"Don't make fun\")\n",
    "doc2 = nlp(\"It's funny\")\n",
    "similarity = doc1.similarity(doc2)\n",
    "print(f\"Text Similarity: {similarity}\\n\")\n",
    "\n",
    "# 8. Visualizing Dependencies\n",
    "print(\"Visualizing Dependencies:\")\n",
    "displacy.render(doc, style=\"dep\", jupyter=False)\n",
    "\n",
    "# 9. Custom Named Entity Recognition\n",
    "print(\"Custom Named Entity Recognition:\")\n",
    "org_label = Span(doc, 7, 8, label=\"ORG\")\n",
    "money_label = Span(doc, 12, 15, label=\"MONEY\")\n",
    "doc.ents = list(doc.ents) + [org_label, money_label]\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text} - {ent.label_}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# 10. Rule-Based Matching\n",
    "print(\"Rule-Based Matching:\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"LOWER\": \"nlp\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"library\"}]\n",
    "matcher.add(\"NLP_LIBRARY_PATTERN\", [pattern])\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(span.text)\n",
    "print(\"\\n\")\n",
    "\n",
    "# 11. Sentence Segmentation\n",
    "print(\"Sentence Segmentation:\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)\n",
    "print(\"\\n\")\n",
    "\n",
    "# 12. Word Vectors and Similarity\n",
    "print(\"Word Vectors and Similarity:\")\n",
    "token1 = nlp(\"cat\")[0]\n",
    "token2 = nlp(\"dog\")[0]\n",
    "similarity = token1.similarity(token2)\n",
    "print(f\"Word Similarity: {similarity}\\n\")\n",
    "\n",
    "# 13. Custom Components in Pipeline\n",
    "print(\"Custom Components in Pipeline:\")\n",
    "def custom_component(doc):\n",
    "    print(\"Custom component executed\")\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(custom_component, last=True)\n",
    "doc = nlp(\"SpaCy is an excellent NLP library.\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# 14. Extending Token and Doc Objects\n",
    "print(\"Extending Token and Doc Objects:\")\n",
    "Token.set_extension(\"is_custom\", default=False)\n",
    "token = nlp(\"Hello\")[0]\n",
    "token._.is_custom = True\n",
    "print(token._.is_custom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0a3423a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing Dependencies:\n",
      "Custom Named Entity Recognition:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E1010] Unable to set entity information for token 7 which is included in more than one span in entities, blocked, missing or outside.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_38604\\2106944983.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0morg_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSpan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ORG\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mmoney_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSpan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"MONEY\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ments\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0morg_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmoney_label\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ment\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ments\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{ent.text} - {ent.label_}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\practice\\lib\\site-packages\\spacy\\tokens\\doc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.ents.__set__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\practice\\lib\\site-packages\\spacy\\tokens\\doc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.set_ents\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E1010] Unable to set entity information for token 7 which is included in more than one span in entities, blocked, missing or outside."
     ]
    }
   ],
   "source": [
    "\n",
    "# 8. Visualizing Dependencies\n",
    "print(\"Visualizing Dependencies:\")\n",
    "displacy.render(doc, style=\"dep\", jupyter=False)\n",
    "\n",
    "# 9. Custom Named Entity Recognition\n",
    "print(\"Custom Named Entity Recognition:\")\n",
    "org_label = Span(doc, 7, 8, label=\"ORG\")\n",
    "money_label = Span(doc, 12, 15, label=\"MONEY\")\n",
    "doc.ents = list(doc.ents) + [org_label, money_label]\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text} - {ent.label_}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# 10. Rule-Based Matching\n",
    "print(\"Rule-Based Matching:\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"LOWER\": \"nlp\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"library\"}]\n",
    "matcher.add(\"NLP_LIBRARY_PATTERN\", [pattern])\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(span.text)\n",
    "print(\"\\n\")\n",
    "\n",
    "# 11. Sentence Segmentation\n",
    "print(\"Sentence Segmentation:\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)\n",
    "print(\"\\n\")\n",
    "\n",
    "# 12. Word Vectors and Similarity\n",
    "print(\"Word Vectors and Similarity:\")\n",
    "token1 = nlp(\"cat\")[0]\n",
    "token2 = nlp(\"dog\")[0]\n",
    "similarity = token1.similarity(token2)\n",
    "print(f\"Word Similarity: {similarity}\\n\")\n",
    "\n",
    "# 13. Custom Components in Pipeline\n",
    "print(\"Custom Components in Pipeline:\")\n",
    "def custom_component(doc):\n",
    "    print(\"Custom component executed\")\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(custom_component, last=True)\n",
    "doc = nlp(\"SpaCy is an excellent NLP library.\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# 14. Extending Token and Doc Objects\n",
    "print(\"Extending Token and Doc Objects:\")\n",
    "Token.set_extension(\"is_custom\", default=False)\n",
    "token = nlp(\"Hello\")[0]\n",
    "token._.is_custom = True\n",
    "print(token._.is_custom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f44f759",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
