{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ddb3448",
   "metadata": {},
   "source": [
    "A Recurrent Neural Network (RNN) is a type of artificial neural network designed for processing sequences of data by utilizing a feedback loop in its architecture. This loop allows information to persist, making RNNs particularly effective for tasks involving sequential or time-series data, such as natural language processing, speech recognition, and time-series forecasting.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Recurrent Connections**:\n",
    "   - Unlike feedforward neural networks, RNNs have connections that form directed cycles. This means the output from a previous step is fed back into the network as input for the current step, enabling the network to maintain a hidden state that captures information from previous steps.\n",
    "\n",
    "2. **Hidden State**:\n",
    "   - The hidden state acts as a memory that captures information about what has been seen in the sequence so far. It is updated at each time step based on the current input and the previous hidden state.\n",
    "\n",
    "3. **Sequence Handling**:\n",
    "   - RNNs process sequences of data by iterating through the sequence one element at a time, updating the hidden state at each step.\n",
    "\n",
    "### How RNNs Work\n",
    "\n",
    "1. **Input Sequence**:\n",
    "   - Given an input sequence \\( x = (x_1, x_2, \\ldots, x_T) \\), where \\( x_t \\) is the input at time step \\( t \\), the network processes each element of the sequence in order.\n",
    "\n",
    "2. **Hidden State Update**:\n",
    "   - At each time step \\( t \\), the hidden state \\( h_t \\) is updated based on the current input \\( x_t \\) and the previous hidden state \\( h_{t-1} \\):\n",
    "     \\[\n",
    "     h_t = \\sigma(W_{xh} x_t + W_{hh} h_{t-1} + b_h)\n",
    "     \\]\n",
    "     Here, \\( W_{xh} \\) is the weight matrix for the input, \\( W_{hh} \\) is the weight matrix for the hidden state, \\( b_h \\) is the bias, and \\( \\sigma \\) is an activation function (typically a non-linear function like tanh or ReLU).\n",
    "\n",
    "3. **Output**:\n",
    "   - The output \\( y_t \\) at each time step can be computed using the hidden state:\n",
    "     \\[\n",
    "     y_t = W_{hy} h_t + b_y\n",
    "     \\]\n",
    "     Where \\( W_{hy} \\) is the weight matrix for the output, and \\( b_y \\) is the bias for the output layer.\n",
    "\n",
    "### Training RNNs\n",
    "\n",
    "RNNs are typically trained using backpropagation through time (BPTT), an extension of the backpropagation algorithm used for feedforward neural networks. BPTT involves unrolling the RNN through time and computing gradients at each time step, which are then used to update the network weights.\n",
    "\n",
    "### Challenges with RNNs\n",
    "\n",
    "1. **Vanishing and Exploding Gradients**:\n",
    "   - During training, gradients can become very small (vanish) or very large (explode), making it difficult for the network to learn long-term dependencies.\n",
    "\n",
    "2. **Difficulty in Capturing Long-Term Dependencies**:\n",
    "   - Standard RNNs struggle with learning dependencies that span long sequences.\n",
    "\n",
    "### Variants of RNNs\n",
    "\n",
    "To address these challenges, several variants of RNNs have been developed:\n",
    "\n",
    "1. **Long Short-Term Memory (LSTM)**:\n",
    "   - LSTMs introduce gates (input, forget, and output gates) to better control the flow of information and mitigate the vanishing gradient problem. They maintain a cell state that carries long-term information across the sequence.\n",
    "\n",
    "2. **Gated Recurrent Unit (GRU)**:\n",
    "   - GRUs are similar to LSTMs but with a simpler architecture, using update and reset gates to manage the flow of information.\n",
    "\n",
    "### Example: Character-Level Language Model\n",
    "\n",
    "Hereâ€™s a simple example of an RNN implemented in Python using TensorFlow/Keras to model sequences of characters:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN\n",
    "\n",
    "# Sample data: sequences of characters (one-hot encoded)\n",
    "data = np.array([\n",
    "    [[1, 0, 0], [0, 1, 0], [0, 0, 1]],  # Sequence \"ABC\"\n",
    "    [[0, 1, 0], [0, 0, 1], [1, 0, 0]]   # Sequence \"BCA\"\n",
    "])\n",
    "\n",
    "# Corresponding targets (shifted sequences)\n",
    "targets = np.array([\n",
    "    [[0, 1, 0], [0, 0, 1], [1, 0, 0]],  # Sequence \"BCA\"\n",
    "    [[0, 0, 1], [1, 0, 0], [0, 1, 0]]   # Sequence \"CAB\"\n",
    "])\n",
    "\n",
    "# Define RNN model\n",
    "model = Sequential([\n",
    "    SimpleRNN(3, input_shape=(3, 3), activation='softmax', return_sequences=True),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Train model\n",
    "model.fit(data, targets, epochs=100)\n",
    "\n",
    "# Predict on a new sequence\n",
    "new_sequence = np.array([[[1, 0, 0], [0, 1, 0], [0, 0, 1]]])  # \"ABC\"\n",
    "predicted_sequence = model.predict(new_sequence)\n",
    "print(predicted_sequence)\n",
    "```\n",
    "\n",
    "This example demonstrates a simple RNN that learns to predict the next character in a sequence. More complex tasks and sequences would typically require more advanced RNN variants like LSTMs or GRUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ad5c0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c17147eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c1e8fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load the IMDB dataset\n",
    "max_features = 10000  # Number of words to consider as features\n",
    "maxlen = 500  # Cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "print(\"Loading data...\")\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59484ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n",
       "       list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 8255, 2, 349, 2637, 148, 605, 2, 8003, 15, 123, 125, 68, 2, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 2, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 2, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
       "       list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n",
       "       ...,\n",
       "       list([1, 11, 6, 230, 245, 6401, 9, 6, 1225, 446, 2, 45, 2174, 84, 8322, 4007, 21, 4, 912, 84, 2, 325, 725, 134, 2, 1715, 84, 5, 36, 28, 57, 1099, 21, 8, 140, 8, 703, 5, 2, 84, 56, 18, 1644, 14, 9, 31, 7, 4, 9406, 1209, 2295, 2, 1008, 18, 6, 20, 207, 110, 563, 12, 8, 2901, 2, 8, 97, 6, 20, 53, 4767, 74, 4, 460, 364, 1273, 29, 270, 11, 960, 108, 45, 40, 29, 2961, 395, 11, 6, 4065, 500, 7, 2, 89, 364, 70, 29, 140, 4, 64, 4780, 11, 4, 2678, 26, 178, 4, 529, 443, 2, 5, 27, 710, 117, 2, 8123, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 1242, 1209, 10, 10, 288, 2260, 1702, 34, 2901, 2, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2766, 234, 1119, 1574, 7, 496, 4, 139, 929, 2901, 2, 7750, 5, 4241, 18, 4, 8497, 2, 250, 11, 1818, 7561, 4, 4217, 5408, 747, 1115, 372, 1890, 1006, 541, 9303, 7, 4, 59, 2, 4, 3586, 2]),\n",
       "       list([1, 1446, 7079, 69, 72, 3305, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 4120, 2959, 45, 58, 1466, 13, 197, 12, 16, 43, 23, 2, 5, 62, 30, 145, 402, 11, 4131, 51, 575, 32, 61, 369, 71, 66, 770, 12, 1054, 75, 100, 2198, 8, 4, 105, 37, 69, 147, 712, 75, 3543, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 1814, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 2, 40, 319, 5872, 112, 6700, 11, 4803, 121, 25, 70, 3468, 4, 719, 3798, 13, 18, 31, 62, 40, 8, 7200, 4, 2, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 2, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 2, 12, 38, 84, 80, 124, 12, 9, 23]),\n",
       "       list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 2, 270, 2, 5, 2, 2, 732, 2098, 101, 405, 39, 14, 1034, 4, 1310, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 4039, 9245, 9, 24, 6, 78, 1099, 17, 2345, 2, 21, 27, 9685, 6139, 5, 2, 1603, 92, 1183, 4, 1310, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 6789, 2, 9, 6, 66, 78, 1099, 4, 631, 1191, 5, 2642, 272, 191, 1070, 6, 7585, 8, 2197, 2, 2, 544, 5, 383, 1271, 848, 1468, 2, 497, 2, 8, 1597, 8778, 2, 21, 60, 27, 239, 9, 43, 8368, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 1791, 72, 7, 51, 6, 1739, 22, 4, 204, 131, 9])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d64ae93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "X_train shape: (25000, 500)\n",
      "X_test shape: (25000, 500)\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 76s 121ms/step - loss: 0.5575 - accuracy: 0.7055 - val_loss: 0.8391 - val_accuracy: 0.6198\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 77s 124ms/step - loss: 0.3574 - accuracy: 0.8464 - val_loss: 0.3878 - val_accuracy: 0.8244\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 76s 122ms/step - loss: 0.3100 - accuracy: 0.8670 - val_loss: 0.4249 - val_accuracy: 0.8072\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 74s 118ms/step - loss: 0.2667 - accuracy: 0.8946 - val_loss: 0.3907 - val_accuracy: 0.8514\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 75s 121ms/step - loss: 0.1987 - accuracy: 0.9237 - val_loss: 0.4056 - val_accuracy: 0.8558\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 73s 116ms/step - loss: 0.1434 - accuracy: 0.9470 - val_loss: 0.4344 - val_accuracy: 0.8406\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 74s 118ms/step - loss: 0.1183 - accuracy: 0.9577 - val_loss: 0.5192 - val_accuracy: 0.8120\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 73s 117ms/step - loss: 0.1239 - accuracy: 0.9528 - val_loss: 0.5542 - val_accuracy: 0.7716\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 76s 122ms/step - loss: 0.1228 - accuracy: 0.9541 - val_loss: 0.5348 - val_accuracy: 0.8226\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 74s 118ms/step - loss: 0.1963 - accuracy: 0.9351 - val_loss: 0.6161 - val_accuracy: 0.8064\n",
      "782/782 [==============================] - 22s 28ms/step - loss: 0.5889 - accuracy: 0.8152\n",
      "Test Loss: 0.5889480710029602\n",
      "Test Accuracy: 0.8151999711990356\n",
      "782/782 [==============================] - 22s 28ms/step\n",
      "Prediction: 0.7138, True Value: 0\n",
      "Prediction: 0.9999, True Value: 1\n",
      "Prediction: 0.6231, True Value: 1\n",
      "Prediction: 0.9778, True Value: 0\n",
      "Prediction: 0.9999, True Value: 1\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), \"train sequences\")\n",
    "print(len(X_test), \"test sequences\")\n",
    "\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "\n",
    "# Build the RNN model\n",
    "model = Sequential([\n",
    "    Embedding(max_features, 32, input_length=maxlen),\n",
    "    SimpleRNN(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=batch_size, validation_split=0.2)\n",
    "model.save('my_rnn_model.h5')\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Test Accuracy: {test_acc}')\n",
    "\n",
    "# Predict using the model\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Display some predictions\n",
    "for i in range(5):\n",
    "    print(f'Prediction: {predictions[i][0]:.4f}, True Value: {y_test[i]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "836309f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n",
      "The predicted sentiment is positive.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Assuming 'model' is your trained RNN model and 'word_index' is the word index obtained during training\n",
    "sentence = \"It;s a comedy movie.\"\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts([sentence])\n",
    "sequence = tokenizer.texts_to_sequences([sentence])\n",
    "padded_sequence = pad_sequences(sequence, maxlen=maxlen)\n",
    "\n",
    "# Predict the sentiment\n",
    "prediction = model.predict(padded_sequence)\n",
    "sentiment = 'positive' if prediction > 0.5 else 'negative'\n",
    "print(f'The predicted sentiment is {sentiment}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c5e31a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
