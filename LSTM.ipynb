{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7feb91bf",
   "metadata": {},
   "source": [
    "Long Short-Term Memory (LSTM) is a type of Recurrent Neural Network (RNN) designed to overcome the limitations of traditional RNNs, particularly the vanishing gradient problem. Here's a high-level explanation of how LSTMs work internally and the differences between RNNs and LSTMs:\n",
    "\n",
    "### LSTM Internal Working:\n",
    "An LSTM unit is composed of a cell and three gates: an input gate, an output gate, and a forget gate³. The cell is responsible for maintaining the state over arbitrary time intervals, and the gates regulate the flow of information into and out of the cell. Here's a simplified view of the operations within an LSTM cell:\n",
    "\n",
    "- **Forget Gate**: Decides what information is discarded from the cell state.\n",
    "- **Input Gate**: Updates the cell state with new information from the current input.\n",
    "- **Output Gate**: Determines what the next hidden state should be, which is used for predictions and passed to the next time step.\n",
    "\n",
    "These gates use sigmoid and tanh activation functions to control the flow of information, allowing the network to learn what to keep or forget over long sequences.\n",
    "\n",
    "### Difference Between RNN and LSTM:\n",
    "The main difference between RNNs and LSTMs lies in their structure and ability to handle sequential data:\n",
    "\n",
    "- **RNNs** are simpler networks that process sequences by maintaining a hidden state that is updated at each time step. However, they struggle with long-term dependencies due to the vanishing gradient problem, where the contribution of information decays geometrically over time, making it hard to learn connections between distant events in a sequence⁶.\n",
    "\n",
    "- **LSTMs** include additional mechanisms (the gates) that allow them to control the flow of information and maintain a memory over longer sequences. This design helps them to remember important information and forget the irrelevant, making them more effective for tasks involving long-term dependencies⁶.\n",
    "\n",
    "In summary, while both RNNs and LSTMs are used for sequential data, LSTMs are better suited for tasks where the sequence has long-range temporal dependencies. They are more complex but provide a solution to the limitations of RNNs in learning from longer sequences.\n",
    "\n",
    "Source: Conversation with Copilot, 9/6/2024\n",
    "(1) Long short-term memory - Wikipedia. https://en.wikipedia.org/wiki/Long_short-term_memory.\n",
    "(2) Main Difference Between RNN and LSTM- (RNN vs LSTM) - theiotacademy. https://www.theiotacademy.co/blog/what-is-the-main-difference-between-rnn-and-lstm/.\n",
    "(3) LSTMs Explained: A Complete, Technically Accurate, Conceptual ... - Medium. https://medium.com/analytics-vidhya/lstms-explained-a-complete-technically-accurate-conceptual-guide-with-keras-2a650327e8f2.\n",
    "(4) What is LSTM? Introduction to Long Short-Term Memory - Analytics Vidhya. https://www.analyticsvidhya.com/blog/2021/03/introduction-to-long-short-term-memory-lstm/.\n",
    "(5) Deep Learning | Introduction to Long Short Term Memory. https://www.geeksforgeeks.org/deep-learning-introduction-to-long-short-term-memory/.\n",
    "(6) Time Series Prediction with LSTM Recurrent Neural Networks in Python .... https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/.\n",
    "(7) RNN vs GRU vs LSTM - Medium. https://medium.com/analytics-vidhya/rnn-vs-gru-vs-lstm-863b0b7b1573.\n",
    "(8) neural networks - What is the difference between LSTM and RNN .... https://ai.stackexchange.com/questions/18198/what-is-the-difference-between-lstm-and-rnn.\n",
    "(9) The Ultimate Showdown: RNN vs LSTM vs GRU – Which is the Best? - Shiksha. https://www.shiksha.com/online-courses/articles/rnn-vs-gru-vs-lstm/.\n",
    "(10) undefined. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b302dc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "# Set the number of words to consider as features\n",
    "max_features = 20000\n",
    "# Cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 100\n",
    "batch_size = 32\n",
    "\n",
    "# Load the IMDB dataset\n",
    "print('Loading data...')\n",
    "(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# Pad sequences to a uniform length\n",
    "input_train = sequence.pad_sequences(input_train, maxlen=maxlen)\n",
    "input_test = sequence.pad_sequences(input_test, maxlen=maxlen)\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(input_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=10,\n",
    "          validation_data=(input_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "score, acc = model.evaluate(input_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9d8a44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
