{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4396538a",
   "metadata": {},
   "source": [
    "LLM stands for \"Large Language Model.\" Large Language Models are advanced types of artificial intelligence designed to understand, generate, and process human language at a sophisticated level. These models are typically built using deep learning techniques, particularly using architectures such as Transformers, which allow them to handle large amounts of text data and capture complex patterns in language.\n",
    "\n",
    "### Key Characteristics of LLMs\n",
    "\n",
    "1. **Size and Scale**:\n",
    "   - LLMs are characterized by their large number of parameters, often numbering in the billions or even trillions. This allows them to store a vast amount of information and capture intricate patterns in data.\n",
    "\n",
    "2. **Training Data**:\n",
    "   - These models are trained on massive datasets comprising text from a variety of sources, such as books, articles, websites, and other digital content. The extensive training data helps them learn the nuances of language, including grammar, context, and semantic meaning.\n",
    "\n",
    "3. **Architecture**:\n",
    "   - Most LLMs are based on the Transformer architecture, which uses self-attention mechanisms to process input data efficiently. This architecture enables the model to handle long-range dependencies in text, making it effective for tasks that require understanding context across many words or sentences.\n",
    "\n",
    "4. **Pretraining and Fine-Tuning**:\n",
    "   - LLMs undergo a two-step training process:\n",
    "     - **Pretraining**: The model is trained on a large corpus of text in an unsupervised manner, learning general language patterns and representations.\n",
    "     - **Fine-Tuning**: The pretrained model is then fine-tuned on specific datasets for particular tasks (e.g., sentiment analysis, translation, question answering) to improve performance on those tasks.\n",
    "\n",
    "5. **Capabilities**:\n",
    "   - LLMs can perform a wide range of natural language processing (NLP) tasks, including text generation, translation, summarization, question answering, and more. Their versatility and performance on various tasks make them valuable tools in many applications.\n",
    "\n",
    "### Examples of LLMs\n",
    "\n",
    "1. **GPT (Generative Pre-trained Transformer)**:\n",
    "   - Developed by OpenAI, GPT models are some of the most well-known LLMs. Versions include GPT-2, GPT-3, and the latest GPT-4. These models are capable of generating coherent and contextually relevant text based on the input they receive.\n",
    "\n",
    "2. **BERT (Bidirectional Encoder Representations from Transformers)**:\n",
    "   - Developed by Google, BERT is designed for understanding the context of words in a sentence in a bidirectional manner. It's particularly effective for tasks like question answering and sentence classification.\n",
    "\n",
    "3. **T5 (Text-To-Text Transfer Transformer)**:\n",
    "   - Also developed by Google, T5 treats every NLP problem as a text-to-text problem, where both the input and output are text strings. This unified framework allows T5 to handle a variety of NLP tasks.\n",
    "\n",
    "4. **RoBERTa (Robustly Optimized BERT Approach)**:\n",
    "   - An improved version of BERT by Facebook AI, RoBERTa modifies the training approach to enhance performance, making it one of the leading models for many NLP benchmarks.\n",
    "\n",
    "### Applications of LLMs\n",
    "\n",
    "1. **Content Creation**:\n",
    "   - LLMs can generate high-quality text for articles, stories, marketing copy, and more, reducing the workload for human writers.\n",
    "\n",
    "2. **Customer Support**:\n",
    "   - Chatbots and virtual assistants powered by LLMs can handle customer inquiries, providing quick and accurate responses.\n",
    "\n",
    "3. **Translation Services**:\n",
    "   - LLMs improve the accuracy and fluency of machine translation systems, bridging language barriers more effectively.\n",
    "\n",
    "4. **Research and Data Analysis**:\n",
    "   - Researchers use LLMs to extract insights from large text datasets, summarize research papers, and generate hypotheses.\n",
    "\n",
    "5. **Personal Assistants**:\n",
    "   - Virtual personal assistants like Google Assistant, Siri, and Alexa use LLMs to understand and respond to user commands in natural language.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Large Language Models represent a significant advancement in AI and NLP, providing powerful tools for understanding and generating human language. Their ability to handle complex tasks and process vast amounts of text data makes them invaluable in various industries and applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37042837",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "model_name = \"distilbert-base-uncased-distilled-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36382451",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def answer_question_with_sliding_window(question, context, max_length=512, stride=256):\n",
    "    inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].squeeze()\n",
    "\n",
    "    # Split input into chunks with overlap\n",
    "    chunk_size = max_length - len(tokenizer.encode(question, add_special_tokens=False)) - 1\n",
    "    chunks = []\n",
    "    for i in range(0, len(input_ids), stride):\n",
    "        end = i + chunk_size\n",
    "        if end > len(input_ids):\n",
    "            end = len(input_ids)\n",
    "        chunks.append(input_ids[i:end])\n",
    "\n",
    "    # Process each chunk\n",
    "    all_answers = []\n",
    "    for chunk in chunks:\n",
    "        chunk = torch.cat((torch.tensor([tokenizer.cls_token_id]), chunk, torch.tensor([tokenizer.sep_token_id])))\n",
    "        outputs = model(chunk.unsqueeze(0))\n",
    "        answer_start_scores = outputs.start_logits\n",
    "        answer_end_scores = outputs.end_logits\n",
    "        answer_start = torch.argmax(answer_start_scores)\n",
    "        answer_end = torch.argmax(answer_end_scores) + 1\n",
    "        answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(chunk[answer_start:answer_end]))\n",
    "        all_answers.append(answer)\n",
    "\n",
    "    # Combine answers from all chunks\n",
    "    combined_answer = ' '.join(all_answers)\n",
    "    return combined_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c67ea725-be16-473a-9ce3-9b7d62f0d818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\suman\\\\Downloads\\\\DS\\\\NLP'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0e067d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'context' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwho is Siva?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Get the answer\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m answer \u001b[38;5;241m=\u001b[39m answer_question_with_sliding_window(question, \u001b[43mcontext\u001b[49m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'context' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the context and question\n",
    "contexts = \"\"\"\n",
    "An injured woman is shown emerging from a cave underneath a huge mountain, carrying an infant. She kills two soldiers pursuing her and attempts to cross a raging river, but slips and starts getting washed away by the current. Facing imminent death, she holds the baby aloft and prays to Lord Shiva, explaining that while she doesn't care about her life, the baby, Mahendra Baahubali, must live, for the sake of Mahishmati kingdom, and the waiting baby's mother. She offers her own life as compensation for her sins. The child is saved by the people of the local Amburi tribe, who reside near the river and worship Lord Shiva. The wife of the tribe's chieftain, Sanga, decides to adopt the boy and names him Siva.\n",
    "\n",
    "Siva grows up to be an ambitious and mischievous child, obsessed with ascending the mountain. Despite Sanga's pleas, he tries many times to scale the cliffs but always fails. As a young man, he is shown to possess superhuman strength when he lifts a Lingam of Lord Siva and places it at the foot of the mountain. A mask then falls from the cliffs, and realizing it possesses feminine features, Siva becomes obsessed with finding the owner of the mask. He finally succeeds in scaling the mountain. Upon reaching the top, he sees a dashing warrior girl named Avantika fighting soldiers. He discovers that she is a member of a local resistance group dedicated to overthrowing the tyrannical king Bhallaladeva of the nearby kingdom Mahishmati, and rescuing their captive Princess Devasena. Siva is immediately smitten with Avantika and secretly follows her. When she discovers Siva, she attacks him, but he outmaneuvers her and returns her mask, progressively disrobing her with each step. Realizing he scaled the entire mountain to find her, she reciprocates his feelings and they make love.\n",
    "\n",
    "After discovering Avantika's cause, Siva pledges to rescue Devasena himself and departs for Mahishmati. He infiltrates the royal palace disguised as a soldier and distracts Bhallaladeva and his guards long enough for him to rescue Devasena. Bhallaladeva sends his son, Bhadra, and the royal family's loyal slave general Kattappa to recapture Devasena. In the ensuing fight, Siva beheads Bhadra as both the Amburi tribe and resistance warriors arrive. Kattappa lunges at Siva, but stops short of attacking him upon seeing his face. He falls into submission at Siva's feet, proclaiming him to be \"Baahubali\".\n",
    "\n",
    "The next morning, Kattappa reveals to Siva that he is actually Mahendra Baahubali, the son of Amarendra Baahubali, a famous invincible warrior prince from Mahishmati, and that the woman who had sacrificed herself to save him was Queen Sivagami, his grandmother. Amarendra Baahubali was born an orphan; his father, King Vikramadeva, died before he was born and his mother died giving birth to him. Bijjaladeva, Vikramadeva's brother and the next in line for the throne, is denied the position due to his scheming nature, and as such Bijjaladeva's wife, Queen Sivagami, assumes power as a caretaker with the intention of raising both her son Bhallaladeva and the orphaned Baahubali in an equal manner to select the next heir to the throne. While the two are raised as brothers and trained rigorously in numerous subjects, Bhallaladeva retains the power-hungry nature of his father while the affable Baahubali becomes beloved by the kingdom.\n",
    "\n",
    "A traitor named Saketa turns out to be a spy for the savage Kaalakeya tribe, known as destroyers of kingdoms. Their chief Inkoshi declares war on Mahishmati, and Lord Bijjaladeva decides that whoever kills Inkoshi will be crowned king. However, after Inkoshi personally insults Sivagami on the battlefield, she demands that he be brought to her alive. Bijjaladeva cunningly arranges better and sophisticated artillery weapons and men fo Bhallaladeva, but Baahubali still defeats more raiders than Bhallaladeva by using innovative tactics and by inspiring his soldiers. With Mahishmati soldiers flagging in the battle, Amarendra inspires them to not give up in the face of death while also rescuing villagers captured by the Kaalakeyas at a risk. Bhallaladeva, meanwhile, kills Kaalakeyas and villagers indiscriminately, killing Inkoshi as well. Baahubali's valour and concern for the people of his kingdom convinces Sivagami to make Baahubali the heir apparent while Bhallaladeva is made the commander-in-chief for his sheer prowess.\n",
    "\n",
    "In the present day, Siva's adoptive parents, impressed by Kattappa's story, wish to meet Baahubali. A dejected Kattappa explains that Amarendra Baahubali is dead, stabbed behind his back by a traitor, and upon being questioned by Siva, he reveals that the traitor was none other than him.\n",
    "\"\"\"\n",
    "\n",
    "question = \"who is Siva?\"\n",
    "\n",
    "# Get the answer\n",
    "answer = answer_question_with_sliding_window(question, context)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4b63157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, context):\n",
    "    inputs = tokenizer.encode_plus(question, context, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "    # Get model outputs\n",
    "    outputs = model(**inputs)\n",
    "    answer_start_scores = outputs.start_logits\n",
    "    answer_end_scores = outputs.end_logits\n",
    "\n",
    "    # Find the tokens with the highest `start` and `end` scores\n",
    "    answer_start = torch.argmax(answer_start_scores)\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "    # Convert tokens to string\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10268b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are transformers used for?\n",
      "Answer: a type of artificial neural network designed to process sequential data\n"
     ]
    }
   ],
   "source": [
    "# Define the context and question\n",
    "context = \"\"\"\n",
    "Transformers are a type of artificial neural network designed to process sequential data, such as natural language. \n",
    "They use a mechanism called attention to weigh the influence of different parts of the input data, allowing them to \n",
    "handle long-range dependencies more effectively than traditional RNNs. Large Language Models (LLMs) like GPT-4 are \n",
    "built using transformer architecture and are pre-trained on vast amounts of text data to perform a variety of natural \n",
    "language processing tasks.\n",
    "\"\"\"\n",
    "\n",
    "question = \"What are transformers used for?\"\n",
    "\n",
    "# Get the answer\n",
    "answer = answer_question(question, context)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "428322b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.marian.modeling_marian because of the following error (look up to see its traceback):\ncannot import name 'TypeAlias' from 'typing_extensions' (C:\\Users\\suman\\.conda\\envs\\practice\\lib\\site-packages\\typing_extensions.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\.conda\\envs\\practice\\lib\\site-packages\\transformers\\utils\\import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1085\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1086\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1087\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\practice\\lib\\importlib\\__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\practice\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\practice\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\practice\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\practice\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\practice\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\practice\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\practice\\lib\\site-packages\\transformers\\models\\marian\\modeling_marian.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\practice\\lib\\site-packages\\torch\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mset_rng_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_rng_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmanual_seed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_seed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mserialization\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_tensor_str\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mset_printoptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\practice\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBinaryIO\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mType\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIO\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtyping_extensions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTypeAlias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcopyreg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'TypeAlias' from 'typing_extensions' (C:\\Users\\suman\\.conda\\envs\\practice\\lib\\site-packages\\typing_extensions.py)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_41428\\1990103367.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMarianMTModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMarianTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Load the pre-trained MarianMT model and tokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Helsinki-NLP/opus-mt-en-de'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMarianTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\practice\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\practice\\lib\\site-packages\\transformers\\utils\\import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1075\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1076\u001b[0m             \u001b[0mmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1077\u001b[1;33m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1078\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1079\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"module {self.__name__} has no attribute {name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\practice\\lib\\site-packages\\transformers\\utils\\import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1074\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1075\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1076\u001b[1;33m             \u001b[0mmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1077\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1078\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\practice\\lib\\site-packages\\transformers\\utils\\import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1089\u001b[0m                 \u001b[1;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1090\u001b[0m                 \u001b[1;34mf\" traceback):\\n{e}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1091\u001b[1;33m             ) from e\n\u001b[0m\u001b[0;32m   1092\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1093\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.models.marian.modeling_marian because of the following error (look up to see its traceback):\ncannot import name 'TypeAlias' from 'typing_extensions' (C:\\Users\\suman\\.conda\\envs\\practice\\lib\\site-packages\\typing_extensions.py)"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Load the pre-trained MarianMT model and tokenizer\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-de'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "def translate(texts, src_lang=\"en\", tgt_lang=\"de\"):\n",
    "    # Tokenize the input texts\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    # Perform translation and decode the output\n",
    "    translated = model.generate(**inputs)\n",
    "    translated_texts = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
    "    \n",
    "    return translated_texts\n",
    "\n",
    "# Example texts for translation\n",
    "texts = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"Transformers are a type of artificial neural network designed to process sequential data.\"\n",
    "]\n",
    "\n",
    "# Translate from English to German\n",
    "translated_texts = translate(texts, src_lang=\"en\", tgt_lang=\"de\")\n",
    "for original, translated in zip(texts, translated_texts):\n",
    "    print(f\"Original: {original}\\nTranslated: {translated}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccbe0b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
